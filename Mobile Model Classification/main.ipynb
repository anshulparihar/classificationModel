{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import chi2\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the shape of train and test data\n",
    "train_data.shape\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 20)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price_range\n",
       "1    500\n",
       "2    500\n",
       "3    500\n",
       "0    500\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the class distribution\n",
    "train_data.price_range.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "battery_power    0\n",
       "blue             0\n",
       "clock_speed      0\n",
       "dual_sim         0\n",
       "fc               0\n",
       "four_g           0\n",
       "int_memory       0\n",
       "m_dep            0\n",
       "mobile_wt        0\n",
       "n_cores          0\n",
       "pc               0\n",
       "px_height        0\n",
       "px_width         0\n",
       "ram              0\n",
       "sc_h             0\n",
       "sc_w             0\n",
       "talk_time        0\n",
       "three_g          0\n",
       "touch_screen     0\n",
       "wifi             0\n",
       "price_range      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the missing values\n",
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionaltiy reduction\n",
    "\n",
    "\n",
    "# Encode categorical columns if necessary\n",
    "X = train_data.drop('price_range', axis=1)\n",
    "y = train_data['price_range']\n",
    "chi_scores, p_values = chi2(X, y)\n",
    "feature_importance = pd.Series(chi_scores, index=X.columns)\n",
    "\n",
    "chi2_results = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Chi2_Score': chi_scores,\n",
    "    'P_Value': p_values\n",
    "}).sort_values(by='Chi2_Score', ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "chi2_threshold = 0.05  # Minimum chi-squared score\n",
    "p_value_threshold = 0.05  # Maximum acceptable p-value\n",
    "important_features = chi2_results[\n",
    "    (chi2_results['Chi2_Score'] > chi2_threshold) &\n",
    "    (chi2_results['P_Value'] < p_value_threshold)\n",
    "]['Feature']\n",
    "\n",
    "\n",
    "X = X[important_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaking the training data into train and test for model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Detecting and removing outliers using Z Score\n",
    "\n",
    "\n",
    "# # Apply Z-Score method\n",
    "# threshold = 3\n",
    "# z_scores = X.apply(zscore)\n",
    "\n",
    "# # Identify outliers\n",
    "# outliers_z = (z_scores.abs() > threshold)\n",
    "# outliers_df = X[outliers_z.any(axis=1)]  # Rows where any column has an outlier\n",
    "\n",
    "# X = X[(z_scores.abs() <= threshold).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 12)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training feature: (1600, 12)\n",
      "Shape of testing feature: (400, 12)\n",
      "Shape of training label: (1600,)\n",
      "Shape of training label: (400,)\n"
     ]
    }
   ],
   "source": [
    "# Show the Training and Testing Data\n",
    "print('Shape of training feature:', X_train.shape)\n",
    "print('Shape of testing feature:', X_test.shape)\n",
    "print('Shape of training label:', y_train.shape)\n",
    "print('Shape of training label:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_test, y_test):\n",
    "    from sklearn import metrics\n",
    "\n",
    "    # Predict Test Data \n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # Calculate accuracy, precision, recall, f1-score, and kappa score\n",
    "    acc = metrics.accuracy_score(y_test, y_pred)\n",
    "    prec = metrics.precision_score(y_test, y_pred, average='macro')\n",
    "    rec = metrics.recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = metrics.f1_score(y_test, y_pred, average='macro')\n",
    "    kappa = metrics.cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "    # # Calculate area under curve (AUC)\n",
    "    # y_pred_proba = model.predict_proba(x_test)[::,1]\n",
    "    # fpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\n",
    "    # auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Display confussion matrix\n",
    "    cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return {'acc': acc, 'prec': prec, 'rec': rec, 'f1': f1, 'kappa': kappa, 'cm': cm}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Dev\\Classification\\Mobile Model Classification\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Model : Logistic Regression\n",
      "{'acc': 0.635, 'prec': 0.6477057747291239, 'rec': 0.6355145516058992, 'f1': 0.6376238603761414, 'kappa': np.float64(0.5132562655753827), 'cm': array([[69, 23,  1,  0],\n",
      "       [13, 52, 34,  8],\n",
      "       [ 0, 14, 52, 24],\n",
      "       [ 0,  0, 29, 81]])}\n",
      "For Model : Random Forest\n",
      "{'acc': 0.9025, 'prec': 0.9018153236573496, 'rec': 0.9023456604026396, 'f1': 0.9018665826943368, 'kappa': np.float64(0.8697786236602224), 'cm': array([[90,  3,  0,  0],\n",
      "       [ 3, 96,  8,  0],\n",
      "       [ 0,  6, 76,  8],\n",
      "       [ 0,  0, 11, 99]])}\n",
      "For Model : SVM\n",
      "{'acc': 0.9575, 'prec': 0.9579199735449736, 'rec': 0.956147851746284, 'f1': 0.9566707387878197, 'kappa': np.float64(0.9431609214617673), 'cm': array([[ 92,   1,   0,   0],\n",
      "       [  4, 103,   0,   0],\n",
      "       [  0,   4,  81,   5],\n",
      "       [  0,   0,   3, 107]])}\n",
      "For Model : Naive Bayes\n",
      "{'acc': 0.8475, 'prec': 0.852814964590375, 'rec': 0.8499384103318381, 'f1': 0.8485587547140727, 'kappa': np.float64(0.7968072083475596), 'cm': array([[90,  3,  0,  0],\n",
      "       [ 5, 82, 20,  0],\n",
      "       [ 0, 11, 73,  6],\n",
      "       [ 0,  0, 16, 94]])}\n",
      "For Model : Decision Tree\n",
      "{'acc': 0.83, 'prec': 0.825345328030235, 'rec': 0.8269135430275013, 'f1': 0.8252955824488677, 'kappa': np.float64(0.7726360839909054), 'cm': array([[87,  6,  0,  0],\n",
      "       [12, 85, 10,  0],\n",
      "       [ 0, 12, 61, 17],\n",
      "       [ 0,  0, 11, 99]])}\n",
      "For Model : Gradient Boosting\n",
      "{'acc': 0.915, 'prec': 0.9136990274223271, 'rec': 0.9152244482814276, 'f1': 0.9142426608391223, 'kappa': np.float64(0.8864840952531989), 'cm': array([[ 90,   3,   0,   0],\n",
      "       [  5,  96,   6,   0],\n",
      "       [  0,   5,  79,   6],\n",
      "       [  0,   0,   9, 101]])}\n"
     ]
    }
   ],
   "source": [
    "# Building Models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train,y_train)\n",
    "    evaluation = evaluate_model(model, X_test, y_test)\n",
    "    print(f'For Model : {name}')\n",
    "    print(evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision function values: [[3.23524796 2.27228418 0.86710269 0.2870077 ]\n",
      " [2.18906505 3.28546928 1.20821515 0.29979483]\n",
      " [3.26965112 2.28686089 0.84896711 0.30182303]\n",
      " [0.74436303 3.274159   2.25418042 0.27331789]\n",
      " [0.29650972 0.74481727 2.26564386 3.29361762]\n",
      " [0.28397133 2.22212174 3.27898903 0.80682942]\n",
      " [2.21273226 3.28737816 1.17309134 0.30032207]\n",
      " [0.29582609 0.76243667 2.27056565 3.28609988]\n",
      " [0.30128365 0.74091581 2.27402368 3.29737384]\n",
      " [1.88882013 3.27910568 1.1915854  0.28570104]]\n"
     ]
    }
   ],
   "source": [
    "# From above we can see that SVM is the best performing model\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# For SVM (support vector machines), we can access decision function values\n",
    "decision_values = svm.decision_function(X_test)\n",
    "\n",
    "# Print decision function values for first 10 samples\n",
    "print(\"Decision function values:\", abs(decision_values[:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1967    1\n",
       "1332    1\n",
       "1263    0\n",
       "516     1\n",
       "1491    3\n",
       "       ..\n",
       "1511    1\n",
       "1821    3\n",
       "1476    2\n",
       "367     3\n",
       "1372    2\n",
       "Name: price_range, Length: 400, dtype: int64"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_mobile_classification.joblib']"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model\n",
    "import joblib\n",
    "joblib.dump(model, 'svm_mobile_classification.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Loading the model\n",
    "import joblib\n",
    "\n",
    "# Load the saved model from the file\n",
    "model = joblib.load('svm_mobile_classification.joblib')\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
